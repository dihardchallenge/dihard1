<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <title>DIHARD Challenge</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <!-- StyleSheet -->
    <link rel="stylesheet" href="./css/bootstrap.css">
    <link rel="stylesheet" href="./css/custom.min.css">
    <link rel="stylesheet" href="./css/flaty.css">
    <!-- SCRIPT -->
    <script src="./js/dihard.js"></script>
</head>

<body>
<div class="page-header" id="banner">
    <center>
        <div class="my4" style="max-width: 69rem;background-color:pink;padding-left:20px;padding-right:20px">
            <h4 class="card-title" align="justify">
                    <br>
         The first DIHARD challenge is now complete. We would like to thank all teams that participated in the challenge and at the special session at Interspeech 2018 for making it a success. For full results as well as system descriptions, please see the <a href="results.html" style="color:blue">results page</a>. Development and test sets are now available from <a href="https://www.ldc.upenn.edu/" style="color:blue">LDC</a>.
         <br><br>
       </h4>
        </div>
    </center>
</div>
    <div class="container">
        <!-- Navbar -->
        <div class="bs-docs-section clearfix">
            <nav class="navbar navbar-expand-lg navbar-dark bg-primary">
                <a class="navbar-brand" href="index.html">DIHARD</a>
                <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarColor01" aria-controls="navbarColor01" aria-expanded="true" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse show" id="navbarColor01">
                    <ul class="navbar-nav mr-auto">
                        <li class="nav-item">
                            <a class="nav-link" href="overview.html">
                                <font size="4">Overview </font>
                            </a>
                        </li>
                        <li class="nav-item active">
                            <a class="nav-link" href="data.html">
                                <font size="4">Data </font><span class="sr-only">(current)</span>
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="instructions.html">
                                <font size="4">Instructions </font>
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="submission.html">
                                <font size="4">Submissions </font>
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="results.html">
                                <font size="4">Results </font>
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="contact.html">
                                <font size="4">Contact Us </font>
                            </a>
                        </li>
                    </ul>
                </div>
            </nav>
        </div>
        <!-- Navbar -->
        <!-- Content -->
        <div class="jumbotron" id="data">
            <h1>Data</h1> This first iteration of the DIHARD challenge selects data from multiple novel sources including previously unexposed data and data previously developed for purposes other than diarization. Annotations have been converted from other formats for some data sources and created anew for other sources.
            <!-- Training data -->
            <hr class="my-4">
            <h3><b>1. Training data </b></h3>
            <!-- Exclusions -->
            <p align="justify">
                DIHARD participants may use any data to train their system, whether publicly available or not, with the exception of the following previously released LDC corpora, from which portions of the evaluation set are drawn:
                <ul>
                    <li>HCRC Map Task Corpus (LDC93S12)</li>
                    <li>DCIEM Map Task Corpus (LDC96S38)</li>
                    <li>MIXER6 Speech (LDC2013S03)</li>
                </ul>
                Portions of MIXER6 have previously been excerpted for use in the NIST <a href="https://www.nist.gov/sites/default/files/documents/itl/iad/mig/NIST_SRE10_evalplan-r6.pdf">SRE10</a> and <a href="https://www.nist.gov/sites/default/files/documents/itl/iad/mig/NIST_SRE12_evalplan-v17-r1.pdf">SRE12</a> evaluation sets, which also may not be used.
            </P>
            <p align="justify">All training data should be thoroughly documented in the <a href="#appendixc">system description document</a> at the end of the challenge. Please also see the provided list of <a href="#appendixb">suggested training corpora</a>.</p>
            <hr class="my-4">
            <!-- Development data -->
            <h3><b>2. Development data </b></h3>
            <p align="justify">
	        The development set is available from LDC as <a href="https://catalog.ldc.upenn.edu/LDC2019S09">LDC2019S09</a> and <a href="https://catalog.ldc.upenn.edu/LDC2019S10">LDC2019S10</a>.
                Speech samples are distributed alongside diarization and reference speech segmentation and may be used for any purpose including system development or training. These samples consist of approximately 19 hours worth of 5-10 minute chunks drawn from the following domains:
            </p>
            <ul>
                <li><i>Child language acquisition recordings</i></li>
                <p align="justify">
                    Previously unexposed recordings of language acquisition in 6-to-18 month olds. The data was collected in the home using a <a href="https://www.lena.org/">LENA</a> recording device as part of <a href="https://homebank.talkbank.org/access/Password/Bergelson.html">SEEDLingS</a>.
                </p>
            </ul>
            <ul>
                <li><i>Supreme Court oral arguments</i></li>
                <p align="justify">
                    Previously unexposed annotation of oral arguments from the 2001 term of the U.S. Supreme Court that were transcribed and manually word-aligned as part of the <a href="http://www.oyez.org">OYEZ</a> project. The original recordings were made using individual table-mounted microphones, one for each participant, which could be switched on and off by the speakers as appropriate. The outputs of these microphones were summed and recorded on a single-channel reel-to-reel analogue tape recorder. Those tapes were later digitized and made available by Jerry Goldman of <a href="http://www.oyez.org">OYEZ</a>.
                </p>
            </ul>
            <ul>
                <li><i>Clinical interviews</i></li>
                <p align="justify">
                    Previously unexposed recordings of Autism Diagnostic Observation Schedule (ADOS) interviews conducted at the Center for Autism Research (CAR) at the Children's Hospital of Philadelphia (CHOP). ADOS is a semi-structured interview in which clinicians attempt to elicit language that differentiates children with Autism Spectrum Disorders from those without (e.g., &#8220;What does being a friend mean to you?&#8221;). All interviews were conducted by CAR with audio recorded from a video camera mounted on a wall approximately 12 feet from the location inside the room where the interview was conducted.
                </p>
                <p align="justify">
                    Note that in order to publish this data, it had to be de-identified by applying a low-pass filter to regions identified as containing personal identifying information (PII). Pitch information in these regions is still recoverable, but the amplitude levels have been reduced relative to the original signal. Filtering was done with a 10th order Butterworth filter with a passband of 0 to 400 Hz. To avoid abrupt transitions in the resulting waveform, the effect of the filter was gradually faded in and out at the beginning and end of the regions using a ramp of 40 ms.
                </p>
            </ul>
            <ul>
                <li><i>Radio interviews</i></li>
                <p align="justify">
                    Previously unexposed recordings of YouthPoint, a late 1970s radio program run by students at the University of Pennsylvania consisting of student-lead interviews with opinion leaders of the era (e.g., Ann Landers, Mark Hamill, Buckminster Fuller, and Isaac Asimov). The recordings were conducted in a studio on open reel tapes and later digitized at LDC.
                </p>
            </ul>
            <ul>
                <li><i>Map tasks</i></li>
                <p align="justify">
                    Previously <b>exposed</b> recordings of subjects involved in map tasks drawn from the DCIEM Map Task Corpus (LDC96S38). Each map task session contains two speakers sitting opposite one another at a table. Each speaker has a map visible only to him and a designated role as either &#8220;Leader&#8221; or &#8220;Follower.&#8221; The Leader has a route marked on his map and is tasked with communicating this route to the Follower so that he may precisely reproduce it on his own map. Though each speaker was recorded on a separate channel via a close-talking microphone, these have been mixed together for the DIHARD releases.
                </p>
            </ul>
            <ul>
                <li><i>Sociolinguistic interviews</i></li>
                <p align="justify">
                    Previously <b>exposed</b> recordings of sociolinguistic interviews drawn from the SLX Corpus of Classic Sociolinguistic Interviews (LDC2003T15). These are field recordings conducted during the 1960s and 1970s by Bill Labov and his students in various locations within the Americas and the United Kingdom.
                </p>
            </ul>
            <ul>
                <li><i>Meeting speech</i></li>
                <p align="justify">
                    Previously <b>exposed</b> recordings of multiparty (3 to 7 participant) meetings drawn from the 2004 Spring NIST Rich Transcription (RT-04S) dev (LDC2007S11) and eval (LDC2007S12) releases. Meetings were recorded at multiple sites (ICSI, NIST, CMU, and LDC), each with a different microphone setup. For DIHARD, a single channel is distributed for each meeting, corresponding to the RT-04S single distant microphone (SDM) condition. Audio files have been trimmed from the original recordings to the 11 minute scoring regions specified in the RT-04S <a href="overview.html#uem">un-partitioned evaluation map (UEM)</a> files.
                </p>
                <p align="justify">
                    <b>**NOTE**</b> In some cases the scoring region onsets/offsets from the original sources were found to bisect a speech segment. In such cases, the onset or offset was adjusted to fall in silence adjacent to the relevant turn.
                </p>
            </ul>
            <ul>
                <li><i>Audiobooks</i></li>
                <p align="justify">
                    Previously unexposed single-speaker, amateur recordings of audiobooks selected from <a href="https://librivox.org/">LibriVox</a>. In this case, the recordings are unexposed in the sense that while the audio and text these segments were selected from are obviously online and available from LibriVox, they have not previously been released as part of a speech recognition corpus. In particular, care was taken to ensure that the chapters and speakers drawn from were not present in <a href="http://www.openslr.org/12/">LibriSpeech</a>.
                </p>
            </ul>
            <ul>
                <li><i>YouTube videos</i></li>
                <p align="justify">
                    Previously unexposed annotations of web video collected as part of the Video Annotation for Speech Technologies (VAST) project. This domain is expected to be particularly challenging as the videos present a diverse set of topics and recording conditions. Unlike the other sources, which contain only English speech, though not necessarily from native speakers, the VAST selections contain both English and Mandarin speech with half the selections coming from monolingual English videos and half from monolingual Mandarin videos.
                </p>
            </ul>
            <p align="justify">
                All samples are distributed as 16 kHz, mono-channel FLAC files.
            </p>
            <hr class="my-4">
            <!-- Evaluation data -->
            <h3><b>3. Evaluation data </b></h3>
            <p align="justify">
                The evaluation set is available from LDC as <a href="https://catalog.ldc.upenn.edu/LDC2019S12">LDC2019S12</a> and <a href="https://catalog.ldc.upenn.edu/LDC2019S13">LDC2019S13</a>. It consists of approximately 21 hours worth of 5-10 minute speech samples drawn from the same domains and sources as the development set with the following exceptions:
            </p>
            <ul>
                <li><i>Sociolinguistic interviews</i></li>
                <p align="justify">
                    Instead of SLX, previously <b>exposed</b> sociolinguistic interviews recorded as part of MIXER6 (LDC2013S03) are used. While these recordings have not previously been released with diarization or SAD, the audio data was released as part of LDC2013S03, excerpts of which were used in the NIST SRE10 and SRE12 evaluation sets. The released audio comes from microphone five, a PZM microphone.
                </p>
            </ul>
            <ul>
                <li><i>Meeting speech</i></li>
                <p align="justify">
                    For the meeting speech domain, previously unexposed recordings of multiparty (3 to 6 participant) meetings conducted at LDC in the Fall of 2001 as part of ROAR are used. All meetings were recorded in the same room, though with different microphone setups. A single centrally located distant microphone is provided for each meeting.
                </p>
            </ul>
            <ul>
                <li><i>Restaurant conversation</i></li>
                <p align="justify">
                    The evaluation set includes a novel domain, unseen in the development set, consisting of previously unexposed recordings from LDC's Conversations in Restaurants (CIR) collection. These recordings consist of conversations between 3 to 6 speakers, all LDC employees, seated at the same table at a restaurant on the University of Pennsylvania campus. All recordings were conducted using binaural microphones mounted on either side of one speaker's head, whose outputs were then mixed down to one channel.
                </p>
            </ul>
            <p align="justify">
                The domain from which each sample is drawn was not provided during the evaluation period.
            </p>
            <!-- Segmentation -->
            <hr class="my-4">
            <h3><b>4. Segmentation</b></h3>
            <p align="justify">
                Where transcription exists and forced alignment was feasible, initial segment boundaries were produced by refining the human marked boundaries with forced alignment by trimming of turn-initial/turn-final silence and splitting on pauses &gt 200 ms in duration, where for a given speaker, a pause is defined as any segment in which that speaker is not producing a vocalization. This includes breaths, but not coughs, laughs, or lipsmacks. In some cases, during the annotation process non-speech vocal noises were encountered that could not be accurately assigned to a speaker. All such segments have been omitted. Ideally, this segmentation was then checked and corrected by human annotators using a tool equipped with a spectrogam display. Where forced alignment was not possible, manually assigned segment boundaries were used. The reference speech-activity segmentation (SAD) was then derived from the diarization speaker-segment boundaries by merging overlapping segments and removing speaker identification.
            </p>
            <p align="justify">
                Because this was an unfunded pilot project, created under time pressure by volunteers, the full three-step workflow (transcription, alignment, checking and correction by human annotators) could not be implemented for all sources. The situation for each source is as follows:
            </p>
            <ul>
                <li><i>ADOS</i></li>
                <p align="justify">
                    For the selections from &#8220;Autism Diagnostic Observation Schedule (ADOS)&#8221; interviews, the full workflow was implemented.
                </p>
                <li><i>Conversations in Restaurants (CIR)</i></li>
                <p align="justify">
                    For the selections from &#8220;Conversations In Restaurants (CIR)&#8221;, segments were derived from a careful turn-level transcription, without alignment and checking.
                </p>
                <li><i>DCIEM</i></li>
                <p align="justify">
                    For the selections from the (Canadian) &#8220;Defence and Civil Institute of Environmental Medicine (DCIEM)&#8221; map task corpus, the full workflow was implemented.
                </p>
                <li><i>LibriVox</i></li>
                <p align="justify">
                    For the selections from LibriVox audiobooks, the full workflow was implemented.
                </p>
                <li><i>MIXER6</i></li>
                <p align="justify">
                    For the selections from sociolinguistic interviews conducted as part of MIXER 6, the full workflow was implemented.
                </p>
                <li><i>ROAR</i></li>
                <p align="justify">
                    For the selections from meeting data collected at LDC in 2001 as part of the ROAR project, the full workflow was implemented.
                </p>
                <li><i>SCOTUS</i></li>
                <p align="justify">
                    For the selections from 2001 U.S. Supreme Court oral arguments, the full workflow was implemented.
                </p>
                <li><i>SEEDLingS</i></li>
                <p align="justify">
                    For the selections from child language recordings collected as part of the SEEDLingS project, segments were derived from manual segmentation done at LDC (with not entirely consistent guidelines). The evaluation set received an extra QC pass not performed for the development set and, consequently, should be of higher quality, though still imperfect.
                </p>
                <li><i>VAST</i></li>
                <p align="justify">
                    For the selections from the &#8220;Video Annotation for Speech Technologies (VAST)&#8221; project, segments were derived from a careful turn-level transcription performed for that project, without additional alignment and checking.
                </p>
                <li><i>YouthPoint</i></li>
                <p align="justify">
                    For the selections from YouthPoint radio interviews, the full workflow was implemented.
                </p>
                <li><i>RT-04S</i></li>
                <p align="justify">
                    For the selections of meeting speech from LDC2007S1 and LDC2007S12, segments were derived from the original releases' RTTM files without any checking. These files have known issues such as overlapping turns, untranscribed speech, and speech that is inaudible on the distant microphones, which were not corrected.
                </p>
                <li><i>SLX</i></li>
                <p align="justify">
                    For the selections of sociolinguistic interviews drawn from LDC2003T15, the full workflow was implemented.
                </p>
                <p align="justify">
                    We should also note that in cases where recordings from individual microphones were available, transcription and segmentation may have been done separately for each speaker using their individual microphone. This means that the reference RTTM may contain some segments that are inaudible, or nearly so, in the the released single-channel FLAC file, which may be taken from a single distant microphone. This affects MIXER6, ROAR, and (in the Dev set) RT-04S.
                </p>
            </ul>
            <!-- File formats -->
            <hr class="my-4">
            <h3><b>5. File formats </b></h3>
            <p align="justify">For each recording, speech segmentation is provided via an HTK label file listing one segment per line, each line consisting of three space-delimited fields:</p>
            <ul>
                <li>segment onset in seconds from beginning of recording</li>
                <br>
                <li>segment offset in seconds from beginning of recording</li>
                <br>
                <li>segment label (always &#8220;speech&#8221;)</li>
            </ul>
            <p align="justify">For example:</p>
            <pre><code>
    0.10  1.41  speech
    1.98  3.44  speech
    5.0   7.52  speech</code>
        </pre>
            <p align="justify">Following prior NIST RT evaluations, diarization for recordings is provided using Rich Transcription Time Marked (RTTM) files. RTTM files are space-separated text files containing one turn per line, each line containing ten fields:</p>
            <ul>
                <li>Type – segment type; should always by “SPEAKER”</li>
                <br>
                <li>File ID – file name; basename of the recording minus extension (e.g., “rec1 a”)
                </li>
                <br>
                <li>Channel ID – channel (1-indexed) that turn is on; should always be “1”</li>
                <br>
                <li>Turn Onset – onset of turn in seconds from beginning of recording</li>
                <br>
                <li>Turn Duration – duration of turn in seconds</li>
                <br>
                <li>Orthography Field – should always by “&#60;NA&#62;”</li>
                <br>
                <li>Speaker Type – should always be “&#60;NA&#62;”</li>
                <br>
                <li>Speaker Name – name of speaker of turn; should be unique within scope of each file</li>
                <br>
                <li>Confidence Score – system confidence (probability) that information is correct; should always be “&#60;NA&#62;”</li>
                <br>
                <li>Signal Lookahead Time – should always be “&#60;NA&#62;”</li>
                <br>
            </ul>
            <p align="justify">For instance:</p>
            <pre>
              <code>
                SPEAKER CMU 20020319-1400 d01 NONE 1 130.430000 2.350 &#60;NA&#62; &#60;NA&#62; juliet &#60;NA&#62; &#60;NA&#62;
                SPEAKER CMU 20020319-1400 d01 NONE 1 157.610000 3.060 &#60;NA&#62; &#60;NA&#62; tbc &#60;NA&#62; &#60;NA&#62;
                SPEAKER CMU 20020319-1400 d01 NONE 1 130.490000 0.450 &#60;NA&#62; &#60;NA&#62; chek &#60;NA&#62; &#60;NA&#62;</code>
            </pre>
            <!-- Training resources -->
            <hr class="my-4">
            <div id="appendixb">
                <h3><b>Data Resources for Training </b></h3>
                <p align="justify">This identifies a (non-exhaustive) list of publicly available corpora suitable for system training.</p>
                <br>
                <p align="justify">
                    <font size="4"><b>Corpora containing meeting speech</b></font>
                    <br>
                    <br>
                    <i>LDC corpora</i>
                </p>
                <ul>
                    <li>ICSI Meeting Speech Speech (LDC2004S02)</li>
                    <br>
                    <li>ICSI Meeting Transcripts (LDC2004T04)</li>
                    <br>
                    <li>ISL Meeting Speech Part 1 (LDC2004S05)</li>
                    <br>
                    <li>ISL Meeting Transcripts Part 1 (LDC2004T10)</li>
                    <br>
                    <li>NIST Meeting Pilot Corpus Speech (LDC2004S09)</li>
                    <br>
                    <li>NIST Meeting Pilot Corpus Transcripts and Metadata (LDC2004T13)</li>
                    <br>
                    <li>2004 Spring NIST Rich Transcription (RT-04S) Development Data (LDC2007S11)</li>
                    <br>
                    <li>2004 Spring NIST Rich Transcription (RT-04S) Evaluation Data (LDC2007S12)</li>
                    <br>
                    <li>2006 NIST Spoken Term Detection Development Set (LDC2011S02)</li>
                    <br>
                    <li>2006 NIST Spoken Term Detection Evaluation Set (LDC2011S03)</li>
                    <br>
                    <li>2005 Spring NIST Rich Transcription (RT-05S) Evaluation Set (LDC2011S06)</li>
                    <br>
                </ul>
                <p align="justify">
                    <i>Non-LDC corpora</i>
                </p>
                <ul>
                    <li>Augmented Multiparty Interaction (AMI) Meeting Corpus (<a href="http://groups.inf.ed.ac.uk/ami/corpus/">http://groups.inf.ed.ac.uk/ami/corpus/</a>)</li>
                </ul>
                <br>
                <br>
                <p align="justify">
                    <font size="4"><b>Conversational telephone speech (CTS) corpora</b></font>
                    <br>
                    <br>
                    <i>LDC corpora</i>
                </p>
                <ul>
                    <li>CALLHOME Mandarin Chinese Speech (LDC96S34)</li>
                    <br>
                    <li>CALLHOME Spanish Speech (LDC96S35)</li>
                    <br>
                    <li>CALLHOME Japanese Speech (LDC96S37)</li>
                    <br>
                    <li>CALLHOME Mandarin Chinese Transcripts (LDC96T16)</li>
                    <br>
                    <li>CALLHOME Spanish Transcripts (LDC96T17)</li>
                    <br>
                    <li>CALLHOME Japanese Transcripts (LDC96T18)</li>
                    <br>
                    <li>CALLHOME American English Speech (LDC97S42)</li>
                    <br>
                    <li>CALLHOME German Speech (LDC97S43)</li>
                    <br>
                    <li>CALLHOME Egyptian Arabic Speech (LDC97S45)</li>
                    <br>
                    <li>CALLHOME American English Transcripts (LDC97T14)</li>
                    <br>
                    <li>CALLHOME German Transcripts (LDC97T15)</li>
                    <br>
                    <li>CALLHOME Egyptian Arabic Transcripts (LDC97T19)</li>
                    <br>
                    <li>CALLHOME Egyptian Arabic Speech Supplement (LDC2002S37)</li>
                    <br>
                    <li>CALLHOME Egyptian Arabic Transcripts Supplement (LDC2002T38)</li>
                    <br>
                    <li>Switchboard-1 Release 2 (LDC97S62)</li>
                    <br>
                    <li>Fisher English Training Speech Part 1 Speech (LDC2004S13)</li>
                    <br>
                    <li>Fisher English Training Speech Part 1 Transcripts (LDC2004T19)</li>
                    <br>
                    <li>Arabic CTS Levantine Fisher Training Data Set 3, Speech (LDC2005S07)</li>
                    <br>
                    <li>Fisher English Training Part 2, Speech (LDC2005S13)</li>
                    <br>
                    <li>Arabic CTS Levantine Fisher Training Data Set3, Transcripts (LDC2005T03)</li>
                    <br>
                    <li>Fisher English Training Part 2, Transcripts (LDC2005T19)</li>
                    <br>
                    <li>Fisher Levantine Arabic Conversational Telephone Speech (LDC2007S02)</li>
                    <br>
                    <li>Fisher Levantine Arabic Conversational Telephone Speech, Transcripts (LDC2007T04)</li>
                    <br>
                    <li>Fisher Spanish Speech (LDC2010S01)</li>
                    <br>
                    <li>Fisher Spanish - Transcripts (LDC2010T04)</li>
                    <br>
                </ul>
                <br>
                <p align="justify">
                    <font size="4"><b>Other corpora</b></font>
                    <br>
                    <br>
                    <i>LDC corpora</i>
                    <ul>
                        <li>Speech in Noisy Environments (SPINE) Training Audio (LDC2000S87)</li>
                        <br>
                        <li>Speech in Noisy Environments (SPINE) Evaluation Audio (LDC2000S96)</li>
                        <br>
                        <li>Speech in Noisy Environments (SPINE) Training Transcripts (LDC2000T49)</li>
                        <br>
                        <li>Speech in Noisy Environments (SPINE) Evaluation Transcripts (LDC2000T54)</li>
                        <br>
                        <li>Speech in Noisy Environments (SPINE2) Part 1 Audio (LDC2001S04)</li>
                        <br>
                        <li>Speech in Noisy Environments (SPINE2) Part 2 Audio (LDC2001S06)</li>
                        <br>
                        <li>Speech in Noisy Environments (SPINE2) Part 3 Audio (LDC2001S08)</li>
                        <br>
                        <li>Speech in Noisy Environments (SPINE2) Part1 Transcripts (LDC2001T05)</li>
                        <br>
                        <li>Speech in Noisy Environments (SPINE2) Part2 Transcripts (LDC2001T07)</li>
                        <br>
                        <li>Speech in Noisy Environments (SPINE2) Part3 Transcripts (LDC2001T09)</li>
                        <br>
                        <li>Santa Barbara Corpus of Spoken American English Part I (LDC2000S85)</li>
                        <br>
                        <li>Santa Barbara Corpus of Spoken American English Part II (LDC2003S06)</li>
                        <br>
                        <li>Santa Barbara Corpus of Spoken American English PartIII (LDC2004S10)</li>
                        <br>
                        <li>Santa Barbara Corpus of Spoken American English PartIV (LDC2005S25)</li>
                        <br>
                        <li>HAVIC Pilot Transcription (LDC2016V01)</li>
                        <br>
                    </ul>
                    <p align="justify">
                        <i>Non-LDC corpora</i>
                    </p>
                    <ul>
                        <li>LibriSpeech (<a href="http://www.openslr.org/12/">http://www.openslr.org/12/</a>)</li>
                        <li>VoxCeleb (<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/">http://www.robots.ox.ac.uk/~vgg/data/voxceleb/</a>)</li>
                    </ul>
                    <br>
                    <br>
                    <p align="center">
                        <font size="5"> [<a href="#data">back to top</a>]</font>
                    </p>
                    <br>
            </div>
            <!-- System description -->
            <hr class="my-4">
            <div id="appendixc">
                <h3><b>System Descriptions </b></h3>
                <p align="justify">
                    Proper interpretation of the evaluation results requires thorough documentation of each system. Consequently, at the end of the evaluation researchers must submit a full description of their system with sufficient detail for a fellow researcher to understand the approach and data/computational requirements. An acceptable system description should include the following information:
                    <ul>
                        <li>Abstract</li>
                        <br>
                        <li>Data resources</li>
                        <br>
                        <li>Detailed description of algorithm</li>
                        <br>
                        <li>Hardware requirements</li>
                    </ul>
                    <br>
                    <h4><b>Section 1: Abstract</b></h4>
                    <p align="justify">
                        A short (a few sentences) high-level description of the system.</p>
                    <br>
                    <h4><b>Section 2: Data resources</b></h4>
                    <p align="justify">
                        This section should describe the data used for training including both volumes and sources. For LDC or ELRA corpora, catalog ids should be supplied. For other publicly available corpora (e.g., AMI) a link should be provided. In cases where a non-publicly available corpus is used, it should be described in sufficient detail to get the gist of its composition. If the system is composed of multiple components and different components are trained using different resources, there should be an accompanying description of which resources were used for which components.
                    </p>
                    <br>
                    <h4><b>Section 3: Detailed description of algorithm</b></h4>
                    <p align="justify">
                        Each component of the system should be described in sufficient detail that another researcher would be able to reimplement it. You may be brief or omit entirely description of components that are standard (e.g., no need to list the standard equations underlying an LSTM or GRU). If hyperparameter tuning was performed, there should be detailed description both of the tuning process and the final hyperparameters arrived at.
                        <br>
                        <br> We suggest including subsections for each major phase in the system. Suggested subsections:
                    </p>
                    <ul>
                        <li>signal processing – e.g., signal enhancement, denoising, source separation</li>
                        <br>
                        <li>acoustic features – e.g., MFCCs, PLPs, mel fiterbank, PNCCs, RASTA, pitch extraction</li>
                        <br>
                        <li>speech activity detection details – relevant for Track 2 only</li>
                        <br>
                        <li>segment representation – e.g., i-vectors, d-vectors</li>
                        <br>
                        <li>speaker estimation – how number of speakers was estimated if such estimation was performed</li>
                        <br>
                        <li>clustering method – e.g., k-means, agglomerative</li>
                        <br>
                        <li>resegmentation details</li>
                        <br>
                    </ul>
                    <h4><b>Section 4: Hardware requirements</b></h4>
                    <p align="justify">System developers should report the hardware requirements for both training and at test time:</p>
                    <ul>
                        <li>Total number of CPU cores used</li>
                        <br>
                        <li>Description of CPUs used (model, speed, number of cores)</li>
                        <br>
                        <li>Total number of GPUs used</li>
                        <br>
                        <li>Description of used GPUs (model, single precision TFLOPS, memory)</li>
                        <br>
                        <li>Total available RAM</li>
                        <br>
                        <li>Used disk storage</li>
                        <br>
                        <li>Machine learning frameworks used (e.g., PyTorch, Tensorflow, CNTK, etc)</li>
                    </ul>
                    <p align="justify">
                        System execution times to process a single 10 minute recording must be reported.
                    </p>
                    <br>
                    <p align="center">
                        <font size="5"> [<a href="#data">back to top</a>]</font>
                    </p>
                    <br>
            </div>
        </div>
        <!-- Content -->
    </div>
</body>

</html>
